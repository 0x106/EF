Term frequency[edit]
Suppose we have a set of English text documents and wish to determine which document is most relevant to the query "the brown cow". A simple way to start out is by eliminating documents that do not contain all three words "the", "brown", and "cow", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document and sum them all together; the number of times a term occurs in a document is called its term frequency.

The first form of term weighting is due to Hans Peter Luhn (1957) and is based on the Luhn Assumption:
If White plays at A, the black chain loses its last liberty. It is captured and removed from the board.
A vacant point adjacent to a stone is called a liberty for that stone.[36][nb 4] Stones in a chain share their liberties. A chain of stones must have at least one liberty to remain on the board. When a chain is surrounded by opposing stones so that it has no liberties, it is captured and removed from the board.

The weight of a term that occurs in a document is simply proportional to the term frequency. [2]
Inverse document frequency[edit]
Because the term "the" is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word "the" more frequently, without giving enough weight to the more meaningful terms "brown" and "cow". The term "the" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less common words "brown" and "cow". Hence an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.

Karen Spärck Jones (1972) conceived a statistical interpretation of term specificity called IDF, which became a cornerstone of term weighting:

Life and death situations and issues occur when an area with a group of stones surrounds a small area (<7 points) so that it may not be possible to form two separate independent "eyes". As the board fills up during the course of the game, certain groups will survive, and others may not. A group with a single eye can normally be captured, in the end, by filling first round the outside. The purpose of making two eyes is to prevent this. Novices sometimes interpret making two eyes in a narrow way, and form 'explicit' eyes one by one. This is often the wrong approach, and it is better to play generally to make a territory inside a group out of which two eyes can surely be made, if and when the opponent attacks it. Groups with seven or more points of territory can usually form two eyes when attacked, unless there are structural weaknesses.

Because the loss of a group can mean the loss of the game, and because the efficient use of each move is important, knowing the life and death status of one's own groups (as well as one's opponent's) is an important skill to cultivate, if one is to become a strong player. The correct, accurate plays with which to make a group secure, or to kill the opponent's group, are studied deeply by all strong players.


The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs. [3]

Go is an adversarial game with the objective of surrounding a larger total area of the board with one's stones than the opponent. As the game progresses, the players position stones on the board to map out formations and potential territories. Contests between opposing formations are often extremely complex and may result in the expansion, reduction, or wholesale capture and loss of formation stones.

A basic principle of Go is that a group of stones must have at least one "liberty" to remain on the board. A "liberty" is an open "point" (intersection) bordering the group. An enclosed liberty (or liberties) is called an "eye", and a group of stones with two or more eyes is said to be unconditionally "alive".[14] Such groups cannot be captured, even if surrounded.[15] A group with one eye or no eyes is "dead" and cannot resist eventual capture.[16]

The general strategy is to expand one's territory, attack the opponent's weak groups (groups that can be killed), and always stay mindful of the "life status" of one's own groups.[17][18] The liberties of groups are countable. Situations where mutually opposing groups must capture each other or die are called capturing races, or semeai.[19] In a capturing race, the group with more liberties (and/or better "shape") will ultimately be able to capture the opponent's stones.[19][20] Capturing races and the elements of life or death are the primary challenges of Go.

A player may pass on determining that the game offers no further opportunities for profitable play. The game ends when both players pass,[21] and is then scored. For each player, the number of captured stones komi is subtracted from the number of controlled (surrounded) points in "liberties" or "eyes", and the player with the greater score wins the game.[22] Games may also be won by resignation of the opponent.

Bonsai (盆栽?, "plantings in tray", from bon, a tray or low-sided pot and sai, a planting or plantings, About this sound pronunciation (help·info))[1] is a Japanese art form using trees grown in containers. Similar practices exist in other cultures, including the Chinese tradition of penjing from which the art originated, and the miniature living landscapes of Vietnamese hòn non bộ. The Japanese tradition dates back over a thousand years.

"Bonsai" is a Japanese pronunciation of the earlier Chinese term penzai. A "bon" is a tray-like pot.[2] The word bonsai is often used in English as an umbrella term for all miniature trees in containers or pots. This article focuses on bonsai as defined in the Japanese tradition.

The purposes of bonsai are primarily contemplation (for the viewer) and the pleasant exercise of effort and ingenuity (for the grower).[3] By contrast with other plant cultivation practices, bonsai is not intended for production of food or for medicine. Instead, bonsai practice focuses on long-term cultivation and shaping of one or more small trees growing in a container.

A bonsai is created beginning with a specimen of source material. This may be a cutting, seedling, or small tree of a species suitable for bonsai development. Bonsai can be created from nearly any perennial woody-stemmed tree or shrub species[4] that produces true branches and can be cultivated to remain small through pot confinement with crown and root pruning. Some species are popular as bonsai material because they have characteristics, such as small leaves or needles, that make them appropriate for the compact visual scope of bonsai.

The source specimen is shaped to be relatively small and to meet the aesthetic standards of bonsai. When the candidate bonsai nears its planned final size it is planted in a display pot, usually one designed for bonsai display in one of a few accepted shapes and proportions. From that point forward, its growth is restricted by the pot environment. Throughout the year, the bonsai is shaped to limit growth, redistribute foliar vigor to areas requiring further development, and meet the artist's detailed design.

The practice of bonsai is sometimes confused with dwarfing, but dwarfing generally refers to research, discovery, or creation of plant cultivars that are permanent, genetic miniatures of existing species. Bonsai does not require genetically dwarfed trees, but rather depends on growing small trees from regular stock and seeds. Bonsai uses cultivation techniques like pruning, root reduction, potting, defoliation, and grafting to produce small trees that mimic the shape and style of mature, full-size trees.

The Japanese art of bonsai originated from the Chinese practice of penjing.[7] From the 6th century onwards, Imperial embassy personnel and Buddhist students from Japan visited and returned from mainland China, bringing back souvenirs including container plantings.[8] At least 17 diplomatic missions were sent from Japan to the Tang court between the years 603 and 839.[8]

Japan's historical Shōsōin, which houses 7th, 8th, and 9th-century artifacts including material from Japan's Tempyō period, contains an elaborate miniature tree display dating from this time.[9] This artifact is composed of a shallow wooden tray serving as a base, carved wooden mountain models, and sand portraying a riverine sandbar. The artifact includes small tree sculptures in silver metal, which are meant to be placed in the sand to produce a table-top depiction of a treed landscape. Though this display is closer to the Japanese bonkei display than to a living bonsai, it does reflect the period's interest in miniature landscapes.

From about the year 970 comes the first lengthy work of fiction in Japanese, Utsubo Monogatari (The Tale of the Hollow Tree), which includes this passage: "A tree that is left growing in its natural state is a crude thing. It is only when it is kept close to human beings who fashion it with loving care that its shape and style acquire the ability to move one." The idea, therefore, was already established by this time that natural beauty becomes true beauty only when modified in accordance with a human ideal.[10]

In the medieval period, recognizable bonsai began to appear in handscroll paintings like the Ippen shonin eden (1299).[9] Saigyo Monogatari Emaki was the earliest known scroll to depict dwarfed potted trees in Japan. It dates from the year 1195, in the Kamakura period. Wooden tray and dish-like pots with dwarf landscapes on modern-looking wooden shelf/benches are shown in the 1309 Kasuga-gongen-genki scroll. These novelties show off the owner's wealth and were probably exotics imported from China.[11]

Chinese Chan Buddhist monks also came over to teach at Japan's monasteries, and one of the monks' activities was to introduce political leaders of the day to the various arts of miniature landscapes as ideal accomplishments for men of taste and learning.[12][13]

The c. 1300 rhymed prose essay, Bonseki no Fu (Tribute to Bonseki) written by celebrated priest and master of Chinese poetry, Kokan Shiren (1278–1346), outlined the aesthetic principles for what would be termed bonsai, bonseki and garden architecture itself. At first, the Japanese used miniaturized trees grown in containers to decorate their homes and gardens.[13][14][15]

Criticism of the interest in curiously twisted specimens of potted plants shows up in one chapter of the 243-chapter compilation Tsurezuregusa (c.1331). This work would become a sacred teaching handed down from master to student, through a limited chain of poets (some famous), until it was at last widely published in the early 17th century. Before then, the criticism had only a modest influence on dwarf potted tree culture.

In 1351, dwarf trees displayed on short poles were portrayed in the Boki Ekotoba scroll.[16] Several other scrolls and paintings also included depictions of these kinds of trees. Potted landscape arrangements made during the next hundred years or so included figurines after the Chinese fashion in order to add scale and theme. These miniatures would eventually be considered garnishes decidedly to be excluded by Japanese artists who were simplifying their creations in the spirit of Zen Buddhism.[17]

Go (traditional Chinese: 圍棋; simplified Chinese: 围棋; pinyin: About this sound wéiqí; Japanese: 囲碁; rōmaji: igo[nb 2]; Korean: 바둑; romaja: baduk[nb 3]; literally: "encircling game") is an abstract strategy board game for two players, in which the aim is to surround more territory than the opponent.

The game originated and was invented in ancient China more than 5,500 years ago, and is still the oldest board game continuously played today. It was considered one of the four essential arts of the cultured aristocratic Chinese scholar caste in antiquity. The earliest written reference to the game is generally recognized as the historical annal Zuo Zhuan[2][3] (c. 4th century BC).[4]

Despite its relatively simple rules, Go is the most complex board game ever designed, vastly more complex than chess, possessing more possibilities than the total number of atoms in the visible universe. Go has both a larger board with more scope for play and longer games, and, on average, much more alternatives to consider per move.[5]

The playing pieces are called "stones". One player uses the white stones and the other, black. The players take turns placing the stones on the vacant intersections ("points") of a board with a 19×19 grid of lines. Beginners often play on smaller 9×9 and 13×13 boards,[6] and archaeological evidence shows that the game was played in earlier centuries on a board with a 17×17 grid. However, boards with a 19×19 grid had become standard by the time the game had reached what was then the Imperial Chinese Tributary State of Korea in the 5th century CE and later to what was then the Imperial Chinese Tributary State of Japan in the 7th century CE.[7]

The objective of the game—as the translation of its name implies—is to surround a larger total area of the board than the opponent.[8]

Once placed on the board, stones may not be moved, but stones are removed from the board when captured. Capture happens when a stone or group of stones is surrounded by opposing stones on all orthogonally-adjacent points.[9] The game proceeds until neither player wishes to make another move; the game has no set ending conditions beyond this. When a game concludes, the territory is counted along with captured stones and komi (points added to the score of the player with the white stones as compensation for playing second) to determine the winner.[10] Games may also be terminated by resignation.

As of mid-2008, there were well over 40 million Go players worldwide, the overwhelming majority of them living in East Asia.[11] As of December 2015, the International Go Federation has a total of 75 member countries and four Association Membership organizations in multiple countries.[12]

Two players, Black and White, take turns placing a stone (game piece) of their own color on a vacant point (intersection) of the grid on a Go board. Black plays first. If there is a large difference in skill between the players, the weaker player typically uses Black and is allowed to place two or more stones on the board to compensate for the difference (see Go handicaps). The official grid comprises 19×19 lines, though the rules can be applied to any grid size. 13×13 and 9×9 boards are popular choices to teach beginners.[33] Once placed, a stone may not be moved to a different point.[34]

Vertically and horizontally adjacent stones of the same color form a chain (also called a string or group) that cannot subsequently be subdivided and, in effect, becomes a single larger stone.[35] Only stones immediately connected to one another by the lines on the board create a chain; stones that are diagonally adjacent are not connected. Chains may be expanded by placing additional stones on adjacent intersections, and can be connected together by placing a stone on an intersection that is adjacent to two or more chains of the same color.


If White plays at A, the black chain loses its last liberty. It is captured and removed from the board.
A vacant point adjacent to a stone is called a liberty for that stone.[36][nb 4] Stones in a chain share their liberties. A chain of stones must have at least one liberty to remain on the board. When a chain is surrounded by opposing stones so that it has no liberties, it is captured and removed from the board.

Players are not allowed to make a move that returns the game to the previous position. This rule, called the ko rule, prevents unending repetition.[37] As shown in the example pictured: Black has just played the stone marked 1, capturing a white stone at the intersection marked with a circle. If White were now allowed to play on the marked intersection, that move would capture the black stone marked 1 and recreate the situation before Black made the move marked 1. Allowing this could result in an unending cycle of captures by both players. The ko rule therefore prohibits White from playing at the marked intersection immediately. Instead White must play elsewhere, or pass; Black can then end the ko by filling at the marked intersection, creating a five-stone black chain. If White wants to continue the ko (that specific repeating position), White tries to find a play elsewhere on the board that Black must answer; if Black answers, then White can retake the ko. A repetition of such exchanges is called a ko fight.[38]

While the various rule-sets agree on the ko rule prohibiting returning the board to an immediately previous position, they deal in different ways with the relatively uncommon situation in which a player might recreate a past position that is further removed. See Rules of Go: Repetition for further information.

Suicide[edit]

Under normal rules, White cannot play at A because that point has no liberties. Under the Ing[39] and New Zealand rules,[40] White may play A (a ko threat), leaving an empty three-space eye. Black naturally answers by playing at A, creating two eyes.
A player may not place a stone such that it or its group immediately has no liberties, unless doing so immediately deprives an enemy group of its final liberty. In the latter case, the enemy group is captured, leaving the new stone with at least one liberty.[41] This rule is responsible for the all-important difference between one and two eyes: if a group with only one eye is fully surrounded on the outside, it can be killed with a stone placed in its single eye.

The Ing and New Zealand rules do not have this rule,[42] and there a player might destroy one of its own groups—"commit suicide". This play would only be useful in a limited set of situations involving a small interior space.[43]

The mathematical concept of a Hilbert space, named after David Hilbert, generalizes the notion of Euclidean space. It extends the methods of vector algebra and calculus from the two-dimensional Euclidean plane and three-dimensional space to spaces with any finite or infinite number of dimensions. A Hilbert space is an abstract vector space possessing the structure of an inner product that allows length and angle to be measured. Furthermore, Hilbert spaces are complete: there are enough limits in the space to allow the techniques of calculus to be used.

Hilbert spaces arise naturally and frequently in mathematics and physics, typically as infinite-dimensional function spaces. The earliest Hilbert spaces were studied from this point of view in the first decade of the 20th century by David Hilbert, Erhard Schmidt, and Frigyes Riesz. They are indispensable tools in the theories of partial differential equations, quantum mechanics, Fourier analysis (which includes applications to signal processing and heat transfer)—and ergodic theory, which forms the mathematical underpinning of thermodynamics. John von Neumann coined the term Hilbert space for the abstract concept that underlies many of these diverse applications. The success of Hilbert space methods ushered in a very fruitful era for functional analysis. Apart from the classical Euclidean spaces, examples of Hilbert spaces include spaces of square-integrable functions, spaces of sequences, Sobolev spaces consisting of generalized functions, and Hardy spaces of holomorphic functions.

Geometric intuition plays an important role in many aspects of Hilbert space theory. Exact analogs of the Pythagorean theorem and parallelogram law hold in a Hilbert space. At a deeper level, perpendicular projection onto a subspace (the analog of "dropping the altitude" of a triangle) plays a significant role in optimization problems and other aspects of the theory. An element of a Hilbert space can be uniquely specified by its coordinates with respect to a set of coordinate axes (an orthonormal basis), in analogy with Cartesian coordinates in the plane. When that set of axes is countably infinite, this means that the Hilbert space can also usefully be thought of in terms of infinite sequences that are square-summable. Linear operators on a Hilbert space are likewise fairly concrete objects: in good cases, they are simply transformations that stretch the space by different factors in mutually perpendicular directions in a sense that is made precise by the study of their spectrum.

A vector is what is needed to "carry" the point A to the point B; the Latin word vector means "carrier".[4] It was first used by 18th century astronomers investigating planet rotation around the Sun.[5] The magnitude of the vector is the distance between the two points and the direction refers to the direction of displacement from A to B. Many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors, operations which obey the familiar algebraic laws of commutativity, associativity, and distributivity. These operations and associated laws qualify Euclidean vectors as an example of the more generalized concept of vectors defined simply as elements of a vector space.

Vectors play an important role in physics: velocity and acceleration of a moving object and forces acting on it are all described by vectors. Many other physical quantities can be usefully thought of as vectors. Although most of them do not represent distances (except, for example, position or displacement), their magnitude and direction can be still represented by the length and direction of an arrow. The mathematical representation of a physical vector depends on the coordinate system used to describe it. Other vector-like objects that describe physical quantities and transform in a similar way under changes of the coordinate system include pseudovectors and tensors.

James Clerk Maxwell FRS FRSE (13 June 1831 – 5 November 1879) was a Scottish[2][3] scientist in the field of mathematical physics.[4] His most notable achievement was to formulate the classical theory of electromagnetic radiation, bringing together for the first time electricity, magnetism, and light as manifestations of the same phenomenon. Maxwell's equations for electromagnetism have been called the "second great unification in physics"[5] after the first one realised by Isaac Newton.

With the publication of A Dynamical Theory of the Electromagnetic Field in 1865, Maxwell demonstrated that electric and magnetic fields travel through space as waves moving at the speed of light. Maxwell proposed that light is an undulation in the same medium that is the cause of electric and magnetic phenomena.[6] The unification of light and electrical phenomena led to the prediction of the existence of radio waves.

Maxwell helped develop the Maxwell–Boltzmann distribution, a statistical means of describing aspects of the kinetic theory of gases. He is also known for presenting the first durable colour photograph in 1861 and for his foundational work on analysing the rigidity of rod-and-joint frameworks (trusses) like those in many bridges.

His discoveries helped usher in the era of modern physics, laying the foundation for such fields as special relativity and quantum mechanics. Many physicists regard Maxwell as the 19th-century scientist having the greatest influence on 20th-century physics. His contributions to the science are considered by many to be of the same magnitude as those of Isaac Newton and Albert Einstein.[7] In the millennium poll—a survey of the 100 most prominent physicists—Maxwell was voted the third greatest physicist of all time, behind only Newton and Einstein.[8] On the centenary of Maxwell's birthday, Einstein described Maxwell's work as the "most profound and the most fruitful that physics has experienced since the time of Newton".[9]

Recognising the potential of the young boy, Maxwell's mother Frances took responsibility for James's early education, which in the Victorian era was largely the job of the woman of the house.[22] At eight he could recite long passages of Milton and the whole of the 119th psalm (176 verses). Indeed, his knowledge of scripture was already very detailed; he could give chapter and verse for almost any quotation from the psalms. His mother was taken ill with abdominal cancer and, after an unsuccessful operation, died in December 1839 when he was eight years old. James's education was then overseen by his father and his father's sister-in-law Jane, both of whom played pivotal roles in his life.[22] His formal schooling began unsuccessfully under the guidance of a sixteen-year-old hired tutor. Little is known about the young man John hired to instruct his son, except that he treated the younger boy harshly, chiding him for being slow and wayward.[22] John dismissed the tutor in November 1841 and, after considerable thought, sent James to the prestigious Edinburgh Academy.[23] He lodged during term times at the house of his aunt Isabella. During this time his passion for drawing was encouraged by his older cousin Jemima.[24]


Edinburgh Academy, where Maxwell was schooled.
The ten-year-old Maxwell, having been raised in isolation on his father's countryside estate, did not fit in well at school.[25] The first year had been full, obliging him to join the second year with classmates a year his senior.[25] His mannerisms and Galloway accent struck the other boys as rustic. Having arrived on his first day of school wearing a pair of homemade shoes and a tunic, he earned the unkind nickname of "Daftie".[26] He never seemed to resent the epithet, bearing it without complaint for many years.[27] Social isolation at the Academy ended when he met Lewis Campbell and Peter Guthrie Tait, two boys of a similar age who were to become notable scholars later in life. They remained lifelong friends.[10]

Maxwell was fascinated by geometry at an early age, rediscovering the regular polyhedra before he received any formal instruction.[24] Despite winning the school's scripture biography prize in his second year, his academic work remained unnoticed[24] until, at the age of 13, he won the school's mathematical medal and first prize for both English and poetry.[28]

Maxwell's interests ranged far beyond the school syllabus and he did not pay particular attention to examination performance.[28] He wrote his first scientific paper at the age of 14. In it he described a mechanical means of drawing mathematical curves with a piece of twine, and the properties of ellipses, Cartesian ovals, and related curves with more than two foci. His work Oval Curves was presented to the Royal Society of Edinburgh by James Forbes, a professor of natural philosophy at Edinburgh University,[10][29] but Maxwell was deemed too young to present the work himself.[30] The work was not entirely original, since René Descartes had also examined the properties of such multifocal ellipses in the seventeenth century, but he had simplified their construction.[30]

The society is essentially a discussion group. Meetings are held once a week, traditionally on Saturday evenings, during which one member gives a prepared talk on a topic, which is later thrown open for discussion.

The usual procedure was for members to meet at the rooms of those whose turn it was to present the topic. The host would provide refreshments consisting of coffee and sardines on toast, called "whales".[2] Women first gained acceptance into the society in the 1970s.

The Apostles retain a leather diary of their membership ("the book") stretching back to its founder, which includes handwritten notes about the topics each member has spoken on. It is included in the so-called "Ark", which is a cedar chest containing collection of papers with some handwritten notes from the group's early days, about the topics members have spoken on, and the results of the division in which those present voted on the debate. It was a point of honour that the question voted on should bear only a tangential relationship to the matter debated.[3] The members referred to as the "Apostles" are the active, usually undergraduate members; former members are called "angels". Undergraduates apply to become angels after graduating or being awarded a fellowship. Every few years, amid great secrecy, all the angels are invited to an Apostles' dinner at a Cambridge college. There used to be an annual dinner, usually held in London.

Undergraduates being considered for membership are called "embryos" and are invited to "embryo parties", where members judge whether the student should be invited to join. The "embryos" attend these parties without knowing they are being considered for membership. Becoming an Apostle involves taking an oath of secrecy and listening to the reading of a curse, originally written by Apostle Fenton John Anthony Hort, the theologian, in or around 1851.

Former members have spoken of the lifelong bond they feel toward one another. Henry Sidgwick, the philosopher, wrote of the Apostles in his memoirs that "the tie of attachment to this society is much the strongest corporate bond which I have known in my life."

In the opening stages of the game, players typically establish positions (or "bases") in the corners and around the sides of the board. These bases help to quickly develop strong shapes which have many options for life (self-viability for a group of stones that prevents capture) and establish formations for potential territory.[23] Players usually start in the corners because establishing territory is easier with the aid of two edges of the board.[24] Established corner opening sequences are called "joseki" and are often studied independently.[25]

"Dame" are points that lie in-between the boundary walls of black and white, and as such are considered to be of no value to either side. "Seki" are mutually alive pairs of white and black groups where neither has two eyes. A "ko" (Chinese and Japanese: 劫) is a repeated-position shape that may be contested by making forcing moves elsewhere. After the forcing move is played, the ko may be "taken back" and returned to its original position.[26] Some "ko fights" may be important and decide the life of a large group, while others may be worth just one or two points. Some ko fights are referred to as "picnic kos" when only one side has a lot to lose.[27] The Japanese call it a hanami (flower-viewing) ko.[28]

Playing with others usually requires a knowledge of each player's strength, indicated by the player's rank (30kyu→1kyu|1dan→6dan|1dan pro→9dan pro). A difference in rank may be compensated by a handicap—Black is allowed to place two or more stones on the board to compensate for White's greater strength.[29][30] There are different rule-sets (Japanese, Chinese, AGA, etc.), which are almost entirely equivalent, except for certain special-case positions.

Aside from the order of play (alternating moves, Black moves first or takes a handicap) and scoring rules, there are essentially only two rules in Go:

Rule 1 (the rule of liberty) states that every stone remaining on the board must have at least one open "point" (an intersection, called a "liberty") directly orthogonally adjacent (up, down, left, or right), or must be part of a connected group that has at least one such open point ("liberty") next to it. Stones or groups of stones which lose their last liberty are removed from the board.
Rule 2 (the "ko rule") states that the stones on the board must never repeat a previous position of stones. Moves which would do so are forbidden, and thus only moves elsewhere on the board are permitted that turn.
Almost all other information about how the game is played is a heuristic, meaning it is learned information about how the game is played, rather than a rule. Other rules are specialized, as they come about through different rule-sets, but the above two rules cover almost all of any played game.

Although there are some minor differences between rule sets used in different countries,[31] most notably in Chinese and Japanese scoring rules,[32] these differences do not greatly affect the tactics and strategy of the game.

Except where noted, the basic rules presented here are valid independent of the scoring rules used. The scoring rules are explained separately. Go terms for which there are no ready English equivalent are commonly called by their Japanese names.

Ice hockey is a contact team sport played on ice, usually in a rink, in which two teams of skaters use their sticks to shoot a vulcanized rubber puck into their opponent's net to score points. Ice hockey teams usually consist of six players each: one goaltender, and five players who skate up and down the ice trying to take the puck and score a goal against the opposing team.

A fast-paced, physical sport, ice hockey is most popular in areas of North America (particularly Canada and the northern United States) and northern and eastern Europe. Ice hockey is the official national winter sport of Canada,[1] where the game enjoys immense popularity. In North America, the National Hockey League (NHL) is the highest level for men's hockey and the most popular. The Kontinental Hockey League (KHL) is the highest league in Russia and much of Eastern Europe. The International Ice Hockey Federation (IIHF) is the formal governing body for international ice hockey. The IIHF manages international tournaments and maintains the IIHF World Ranking. Worldwide, there are ice hockey federations in 74 countries.[2]

Ice hockey is believed to have evolved from simple stick and ball games played in the 18th and 19th century United Kingdom and elsewhere. These games were brought to North America and several similar winter games using informal rules were developed, such as "shinny" and "ice polo". The contemporary sport of ice hockey was developed in Canada, most notably in Montreal, where the first indoor hockey game was played on March 3, 1875. Some characteristics of that game, such as the length of the ice rink and the use of a puck, have been retained to this day. Amateur ice hockey leagues began in the 1880s, and professional ice hockey originated around 1900. The Stanley Cup, emblematic of ice hockey club supremacy, was first awarded in 1893 to recognize the Canadian amateur champion and later became the championship trophy of the NHL. In the early 1900s, the Canadian rules were adopted by the Ligue Internationale de Hockey sur Glace, the precursor of the IIHF and the sport was played for the first time in the Olympics in the Olympic Games of 1920.

Chess is a two-player strategy board game played on a chessboard, a checkered gameboard with 64 squares arranged in an eight-by-eight grid. Chess is played by millions of people worldwide, both amateurs and professionals.

Each player begins the game with 16 pieces: one king, one queen, two rooks, two knights, two bishops, and eight pawns. Each of the six piece types moves differently. The most powerful piece is the queen and the least powerful piece is the pawn. The objective is to 'checkmate' the opponent's king by placing it under an inescapable threat of capture. To this end, a player's pieces are used to attack and capture the opponent's pieces, while supporting their own. In addition to checkmate, the game can be won by voluntary resignation by the opponent, which typically occurs when too much material is lost, or if checkmate appears unavoidable. A game may also result in a draw in several ways.

Chess is believed to have originated in India, some time before the 7th century, being derived from the Indian game of chaturanga. Chaturanga is also the likely ancestor of the Eastern strategy games xiangqi, janggi and shogi. The pieces took on their current powers in Spain in the late 15th century; the rules were finally standardized in the 19th century. The first generally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886. Since 1948, the World Championship has been controlled by FIDE, the game's international governing body; the current World Champion is the Norwegian Magnus Carlsen. FIDE also organizes the Women's World Championship, the World Junior Championship, the World Senior Championship, the Blitz and Rapid World Championships and the Chess Olympiad, a popular competition among teams from different nations. There is also a Correspondence Chess World Championship and a World Computer Chess Championship. Online chess has opened amateur and professional competition to a wide and varied group of players. There are also many chess variants, with different rules, different pieces, and different boards.

FIDE awards titles to skilled players, the highest of which is grandmaster. Many national chess organizations also have a title system, however these are not recognised by FIDE. The term "master" may refer to a formal title or may be used more loosely for any skilled player.

Chess is a recognized sport of the International Olympic Committee;[2] some national sporting bodies such as the Spanish Consejo Superior de Deportes also recognize chess as a sport.[3] Chess was included in the 2006 and 2010 Asian Games.

Since the second half of the 20th century, computers have been programmed to play chess with increasing success, to the point where the strongest home computers play chess at a higher level than the best human players. Since the 1990s, computer analysis has contributed significantly to chess theory, particularly in the endgame. The computer IBM Deep Blue was the first machine to overcome a reigning World Chess Champion in a match when it defeated Garry Kasparov in 1997. The rise of strong computer programs (known as "engines") that can be run on hand-held devices has led to increasing concerns about cheating during tournaments.

Deep Blue was a chess-playing computer developed by IBM. It is known for being the first computer chess-playing system to win both a chess game and a chess match against a reigning world champion under regular time controls.

Deep Blue won its first game against a world champion on February 10, 1996, when it defeated Garry Kasparov in game one of a six-game match. However, Kasparov won three and drew two of the following five games, defeating Deep Blue by a score of 4–2. Deep Blue was then heavily upgraded, and played Kasparov again in May 1997. Deep Blue won game six, therefore winning the six-game rematch 3½–2½ and becoming the first computer system to defeat a reigning world champion in a match under standard chess tournament time controls.[1] Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.[2]

Development for Deep Blue began in 1985 with the ChipTest project at Carnegie Mellon University. This project eventually evolved into Deep Thought, at which point the development team was hired by IBM. The project evolved once more with the new name Deep Blue in 1989. Grandmaster Joel Benjamin was also signed on to the development team by IBM.

Carnegie Mellon University (Carnegie Mellon or CMU; /ˈkɑːrnᵻɡi ˈmɛlən/ or /kɑːrˈneɪɡi ˈmɛlən/) is a private research university in Pittsburgh, Pennsylvania.

Founded in 1900 by Andrew Carnegie as the Carnegie Technical Schools, the university became the Carnegie Institute of Technology in 1912 and began granting four-year degrees. In 1967, the Carnegie Institute of Technology merged with the Mellon Institute of Industrial Research to form Carnegie Mellon University.

The university's 143-acre (58 ha) main campus is 3 miles (4.8 km) from Downtown Pittsburgh. Carnegie Mellon has seven colleges and independent schools: the College of Engineering, College of Fine Arts, Dietrich College of Humanities and Social Sciences, Mellon College of Science, Tepper School of Business, H. John Heinz III College and the School of Computer Science. The university also has campuses in Qatar and Silicon Valley, with degree-granting programs in six continents.

Carnegie Mellon consistently ranks in the top 25 in the national U.S. News & World Report rankings.[7] It is home to the world’s first degree-granting Robotics and Drama programs,[8] as well as one of the first Computer Science departments.[9] The university conducted over $703 million in research in 2015.[10]

Carnegie Mellon counts 13,650 students from 114 countries, over 100,000 living alumni and over 5,000 faculty and staff. Past and present faculty and alumni include 19 Nobel Prize Laureates, 19 Members of the American Academy of Arts & Sciences,[11] 72 Members of the National Academies, 114 Emmy Award Winners, 41 Tony Award laureates, 7 Academy Award Winners, and 12 Turing Award winners.[12]

The definition of a university varies widely, even within some countries. Where there is clarification, it is usually set by a government agency. For example:

In Australia, the Tertiary Education Quality and Standards Agency (TEQSA) is Australia's independent national regulator of the higher education sector. Students rights within University are also protected by the Education Services for Overseas Students Act (ESOS).

In the United States there is no nationally standardized definition for the term 'University', although the term has traditionally been used to designate research institutions and was once reserved for doctorate-granting research institutions. Some states, such as Massachusetts, will only grant a school "university status" if it grants at least two doctoral degrees.[63]

In the United Kingdom, the Privy Council is responsible for approving the use of the word "university" in the name of an institution, under the terms of the Further and Higher Education Act 1992.[64]

In India, a new designation deemed universities has been created for institutions of higher education that aren't universities, but work at a very high standard in a specific area of study ("An Institution of Higher Education, other than universities, working at a very high standard in specific area of study, can be declared by the Central Government on the advice of the UGC as an Institution ‘Deemed-to-be-university’"). Institutions that are ‘deemed-to-be-university’ enjoy the academic status and the privileges of a university.[65] Through this provision many schools that are commercial in nature and have been established just to exploit the demand for higher education have sprung up.[66]

In Canada, "college" generally refers to a two-year, non-degree-granting institution, while "university" connotes a four-year, degree-granting institution. Universities may be sub-classified (as in the Macleans rankings) into large research universities with many PhD granting programs and medical schools (for example, McGill University); "comprehensive" universities that have some PhDs but aren't geared toward research (such as Waterloo); and smaller, primarily undergraduate universities (such as St. Francis Xavier).

The Crusades were a series of intermittent Papal sanctioned military campaigns beginning in the late 11th century. They commenced with a call to arms by Pope Urban II who was responding to a request for military support for the Byzantine Empire. The Byzantine Emperor, Alexios I, needed military reinforcements for the conflict with the westward migrating Turks in Anatolia. Historians debate Urban and the Crusader's primary motivations. One of Urban's stated aims was to guarantee pilgrims access to the holy sites in the Holy Land that were under Muslim control while his wider strategy may have been to establish himself as head of the united Church and bringing together the Eastern and Western branches of Christendom that had been divided since their split in 1054. What is known though is the unprecedented response to Urban's preaching and the basis it established for later crusades. Hundreds of thousands of people from many different classes across Western Europe became crusaders by taking a public vow and receiving plenary indulgences from the church. Some were peasants hoping for Apotheosis at Jerusalem. Urban preached that anyone who participated would be forgiven by God of all their sins. In addition some historians argue that participation satisfied feudal obligations and provided opportunities for economic and political gain.

Opinions concerning the conduct of Crusaders have varied from laudatory to highly critical. Crusaders often pillaged the countries through which they traveled, and contrary to their promises the leaders retained much of this territory rather than returning it to the Byzantines. The People's Crusade prompted the murder of thousands of Jews, known as the Rhineland massacres. Constantinople was sacked during the Fourth Crusade, rendering the reunification of Christendom impossible.

The impact of the Crusades was profound; they reopened the Mediterranean to commerce and travel, enabling Genoa and Venice to flourish. The Crusades consolidated the collective identity of the Latin Church under papal leadership, and were a source of heroism, chivalry, and piety. This consequently spawned medieval romance, philosophy, and literature. However, the Crusades reinforced the connection between Western Christendom, feudalism, and militarism.

In Christian theology, divinization (deification, making divine, or theosis) is the transforming effect of divine grace,[1] the spirit of God, or the atonement of Christ. It literally means to become more divine, more like God, or take upon a divine nature.

God in Christianity is the eternal being who created and preserves all things. Christians believe God to be both transcendent (wholly independent of, and removed from, the material universe) and immanent (involved in the world).[1][2] Christian teachings of the immanence and involvement of God and his love for humanity exclude the belief that God is of the same substance as the created universe[3] but accept that God's divine Nature was hypostatically united to human nature in the person of Jesus Christ, in an event known as the Incarnation.

Early Christian views of God were expressed in the Pauline Epistles and the early[4] creeds, which proclaimed one God and the divinity of Jesus, almost in the same breath as in 1 Corinthians (8:5-6): "For even if there are so-called gods, whether in heaven or on earth (as indeed there are many 'gods' and many 'lords'), yet for us there is but one God, the Father, from whom all things came and for whom we live; and there is but one Lord, Jesus Christ, through whom all things came and through whom we live."[5][6][7] "Although the Judæo-Christian sect of the Ebionites protested against this apotheosis of Jesus,[8] the great mass of Gentile Christians accepted it."[9] This began to differentiate the Gentile Christian views of God from traditional Jewish teachings of the time.[5]

The theology of the attributes and nature of God has been discussed since the earliest days of Christianity, with Irenaeus writing in the 2nd century: "His greatness lacks nothing, but contains all things".[10] In the 8th century, John of Damascus listed eighteen attributes which remain widely accepted.[11] As time passed, theologians developed systematic lists of these attributes, some based on statements in the Bible (e.g., the Lord's Prayer, stating that the Father is in Heaven), others based on theological reasoning.[12][13] The Kingdom of God is a prominent phrase in the Synoptic Gospels and while there is near unanimous agreement among scholars that it represents a key element of the teachings of Jesus, there is little scholarly agreement on its exact interpretation.[14][15]

Although the New Testament does not have a formal doctrine of the Trinity as such, it does repeatedly speak of the Father, the Son, and the Holy Spirit in such a way as to "compel a trinitarian understanding of God." This never becomes a tritheism, i.e. this does not imply three Gods.[16] Around the year 200, Tertullian formulated a version of the doctrine of the Trinity which clearly affirmed the divinity of Jesus and came close to the later definitive form produced by the Ecumenical Council of 381.[17][18] The doctrine of the Trinity can be summed up as: "The One God exists in Three Persons and One Substance, as God the Father, God the Son and God the Holy Spirit."[19][20] Trinitarians, who form the large majority of Christians, hold it as a core tenet of their faith.[21][22] Nontrinitarian denominations define the Father, the Son, and the Holy Spirit in a number of different ways.[23]

Ancient Greek includes the forms of Greek used in ancient Greece and the ancient world from around the 9th century BCE to the 6th century CE. It is often roughly divided into the Archaic period (9th to 6th centuries BCE), Classical period (5th and 4th centuries BCE), and Hellenistic period (3rd century BCE to 6th century CE). It is antedated in the second millennium BCE by Mycenaean Greek.

The language of the Hellenistic phase is known as Koine (common). Koine is regarded as a separate historical stage of its own, although in its earliest form it closely resembled Attic Greek and in its latest form it approaches Medieval Greek. Prior to the Koine period, Greek of the classic and earlier periods included several regional dialects.

Ancient Greek was the language of Homer and of classical Athenian historians, playwrights, and philosophers. It has contributed many words to English vocabulary and has been a standard subject of study in educational institutions of the West since the Renaissance. This article primarily contains information about the Epic and Classical phases of the language.

The Renaissance (UK /rᵻˈneɪsəns/, US /rɛnəˈsɑːns/)[1] is a period in Europe, from the 14th to the 17th century, used as the cultural bridge between the Middle Ages and modern history. It started as a cultural movement in Italy in the Late Medieval period and later spread to the rest of Europe, marking the beginning of the Early Modern Age.

The intellectual basis of the Renaissance was its own invented version of humanism, derived from the rediscovery of classical Greek philosophy, such as that of Protagoras, who said that "Man is the measure of all things." This new thinking became manifest in art, architecture, politics, science and literature. Early examples were the development of perspective in oil painting and the recycled knowledge of how to make concrete. Although the invention of metal movable type sped the dissemination of ideas from the later 15th century, the changes of the Renaissance were not uniformly experienced across Europe.

As a cultural movement, the Renaissance encompassed innovative flowering of Latin and vernacular literatures, beginning with the 14th-century resurgence of learning based on classical sources, which contemporaries credited to Petrarch; the development of linear perspective and other techniques of rendering a more natural reality in painting; and gradual but widespread educational reform. In politics, the Renaissance contributed to the development of the customs and conventions of diplomacy, and in science to an increased reliance on observation and inductive reasoning. Although the Renaissance saw revolutions in many intellectual pursuits, as well as social and political upheaval, it is perhaps best known for its artistic developments and the contributions of such polymaths as Leonardo da Vinci and Michelangelo, who inspired the term "Renaissance man".[2][3]

There is a consensus that the Renaissance began in Florence, in the 14th century.[4] Various theories have been proposed to account for its origins and characteristics, focusing on a variety of factors including the social and civic peculiarities of Florence at the time: its political structure; the patronage of its dominant family, the Medici;[5][6] and the migration of Greek scholars and texts to Italy following the Fall of Constantinople to the Ottoman Turks.[7][8][9] Other major centres were northern Italian city-states such as Venice, Genoa, Milan, Bologna, and finally Rome during the Renaissance Papacy.

The Renaissance has a long and complex historiography, and, in line with general scepticism of discrete periodizations, there has been much debate among historians reacting to the 19th-century glorification of the "Renaissance" and individual culture heroes as "Renaissance men", questioning the usefulness of Renaissance as a term and as a historical delineation.[10] The art historian Erwin Panofsky observed of this resistance to the concept of "Renaissance"

Desiderius Erasmus Roterodamus (/ˌdɛzɪˈdɪəriəs ɪˈræzməs/; 28 October[1] 1466[2] – 12 July 1536), known as Erasmus of Rotterdam, or simply Erasmus,[note 1] was a Dutch Renaissance humanist, Catholic priest, social critic, teacher, and theologian.

Erasmus was a classical scholar and wrote in a pure Latin style. Among humanists he enjoyed the sobriquet "Prince of the Humanists", and has been called "the crowning glory of the Christian humanists".[3] Using humanist techniques for working on texts, he prepared important new Latin and Greek editions of the New Testament, which raised questions that would be influential in the Protestant Reformation and Catholic Counter-Reformation. He also wrote On Free Will,[4] The Praise of Folly, Handbook of a Christian Knight, On Civility in Children, Copia: Foundations of the Abundant Style, Julius Exclusus, and many other works.

Erasmus lived against the backdrop of the growing European religious Reformation, but while he was critical of the abuses within the Catholic Church and called for reform, he kept his distance from Luther and Melanchthon and continued to recognise the authority of the pope, emphasizing a middle way with a deep respect for traditional faith, piety and grace, rejecting Luther's emphasis on faith alone. Erasmus remained a member of the Roman Catholic Church all his life,[5] remaining committed to reforming the Church and its clerics' abuses from within. He also held to the Catholic doctrine of free will, which some Reformers rejected in favor of the doctrine of predestination. His middle road ("Via Media") approach disappointed and even angered scholars in both camps.

Erasmus died suddenly in Basel in 1536 while preparing to return to Brabant, and was buried in the Basel Minster, the former cathedral of the city.[6] A bronze statue of him was erected in his city of birth in 1622, replacing an earlier work in stone.

The Protestant Reformation, often referred to simply as the Reformation (Latin: reformatio), was a schism from the Roman Catholic Church initiated by Martin Luther and continued by other early Protestant Reformers in 16th-century Europe.

Although there had been significant earlier attempts to reform the Roman Catholic Church before Luther – such as those of Jan Hus, Peter Waldo, and John Wycliffe – Martin Luther is widely acknowledged to have started the Reformation with his 1517 work The Ninety-Five Theses. Luther began by criticizing the selling of indulgences, insisting that the Pope had no authority over purgatory and that the Catholic doctrine of the merits of the saints had no foundation in the gospel. The Protestant position, however, would come to incorporate doctrinal changes such as sola scriptura and sola fide. The core motivation behind these changes was theological, though many other factors played a part, including the rise of nationalism, the Western Schism which eroded people's faith in the Papacy, the perceived corruption of the Roman Curia, the impact of humanism and the new learning of the Renaissance which questioned much of the traditional thought.

The initial movement within Germany diversified, and other reform impulses arose independently of Luther. The spread of Gutenberg's printing press provided the means for the rapid dissemination of religious materials in the vernacular. The largest groups were the Lutherans and Calvinists. Lutheran churches were founded mostly in Germany, the Baltics and Scandinavia, while the Reformed ones were founded in Switzerland, Hungary, France, the Netherlands and Scotland. The new movement influenced the Church of England decisively after 1547 under Edward VI and Elizabeth I, although the Church of England had been made independent under Henry VIII in the early 1530s for political rather than religious reasons.

There were also reformation movements throughout continental Europe known as the Radical Reformation, which gave rise to the Anabaptist, Moravian, and other Pietistic movements. Radical Reformers, besides forming communities outside state sanction, often employed more extreme doctrinal change, such as the rejection of tenets of the councils of Nicaea and Chalcedon.

The Roman Catholic Church responded with a Counter-Reformation initiated by the Council of Trent. Much work in battling Protestantism was done by the well-organized new order of the Jesuits. In general, Northern Europe, with the exception of most of Ireland, came under the influence of Protestantism. Southern Europe remained Roman Catholic, while Central Europe was a site of a fierce conflict, culminating in the Thirty Years' War, which left it massively devastated.

Penance is repentance of sins as well as the proper name of the Roman Catholic, Eastern Orthodox, and Anglican Sacrament of Penance and Reconciliation/Confession. It also plays a part in non-sacramental confession among Lutherans and other Protestants. The word penance derives from Old French and Latin paenitentia, both of which derive from the same root meaning repentance, the desire to be forgiven (in English see contrition). Penance and repentance, similar in their derivation and original sense, have come to symbolize conflicting views of the essence of repentance, arising from the controversy as to the respective merits of "faith" and "good works". Word derivations occur in many languages.

Forgiveness is the intentional and voluntary process by which a victim undergoes a change in feelings and attitude regarding an offense, lets go of negative emotions such as vengefulness, with an increased ability to wish the offender well.[1][2] Forgiveness is different from condoning (failing to see the action as wrong and in need of forgiveness), excusing (not holding the offender as responsible for the action), forgetting (removing awareness of the offense from consciousness), pardoning (granted by a representative of society, such as a judge), and reconciliation (restoration of a relationship).[1]

In certain contexts, forgiveness is a legal term for absolving or giving up all claims on account of debt, loan, obligation, or other claims.[3][4]

As a psychological concept and virtue, the benefits of forgiveness have been explored in religious thought, the social sciences and medicine. Forgiveness may be considered simply in terms of the person who forgives[5] including forgiving themselves, in terms of the person forgiven or in terms of the relationship between the forgiver and the person forgiven. In most contexts, forgiveness is granted without any expectation of restorative justice, and without any response on the part of the offender (for example, one may forgive a person who is incommunicado or dead). In practical terms, it may be necessary for the offender to offer some form of acknowledgment, an apology, or even just ask for forgiveness, in order for the wronged person to believe himself able to forgive.[1]

Most world religions include teachings on the nature of forgiveness, and many of these teachings provide an underlying basis for many varying modern day traditions and practices of forgiveness. Some religious doctrines or philosophies place greater emphasis on the need for humans to find some sort of divine forgiveness for their own shortcomings, others place greater emphasis on the need for humans to practice forgiveness of one another, yet others make little or no distinction between human and divine forgiveness.

Law is a system of rules that are enforced through social institutions to govern behavior.[2] Laws can be made by a collective legislature or by a single legislator, resulting in statutes, by the executive through decrees and regulations, or by judges through binding precedent, normally in common law jurisdictions. Private individuals can create legally binding contracts, including arbitration agreements that may elect to accept alternative arbitration to the normal court process. The formation of laws themselves may be influenced by a constitution, written or tacit, and the rights encoded therein. The law shapes politics, economics, history and society in various ways and serves as a mediator of relations between people.

A general distinction can be made between (a) civil law jurisdictions (including Catholic canon law and socialist law), in which the legislature or other central body codifies and consolidates their laws, and (b) common law systems, where judge-made precedent is accepted as binding law. Historically, religious laws played a significant role even in settling of secular matters, which is still the case in some religious communities, particularly Jewish, and some countries, particularly Islamic. Islamic Sharia law is the world's most widely used religious law.[3]

The adjudication of the law is generally divided into two main areas referred to as (i) Criminal law and (ii) Civil law. Criminal law deals with conduct that is considered harmful to social order and in which the guilty party may be imprisoned or fined. Civil law (not to be confused with civil law jurisdictions above) deals with the resolution of lawsuits (disputes) between individuals or organizations.

Law provides a rich source of scholarly inquiry into legal history, philosophy, economic analysis and sociology. Law also raises important and complex issues concerning equality, fairness, and justice. There is an old saying that 'all are equal before the law', although Jonathan Swift argued that 'Laws are like cobwebs, which may catch small flies, but let wasps and hornets break through.' In 1894, the author Anatole France said sarcastically, "In its majestic equality, the law forbids rich and poor alike to sleep under bridges, beg in the streets, and steal loaves of bread."[4] Writing in 350 BC, the Greek philosopher Aristotle declared, "The rule of law is better than the rule of any individual."[5] Mikhail Bakunin said: "All law has for its object to confirm and exalt into a system the exploitation of the workers by a ruling class".[6] Cicero said "more law, less justice".[7] Marxist doctrine asserts that law will not be required once the state has withered away.[8] Regardless of one's view of the law, it remains today a completely central institution

In modern usage, a magistrate is a judge or lawyer who hears cases in courts. In the United Kingdom, the term magistrate usually refers to a volunteer of the Magistrates' Courts. Historically in Canada, magistrates were retired police officers, but they are now lawyers appointed by the lieutenant-governor-in-council. Canadian magistrates are known today as provincial court judges. They are judicial officers with summary jurisdiction in both criminal and civil actions, hearing minor indictable offenses and those cases where the accused may elect the mode of trial. They may preside over family court or small-claims court, and are ex-officio commissioners for oaths.[1] In the United States, magistrate judges are appointed to assist United States district court judges in the performance of their duties.[2] Magistrate judges are authorized by 28 U.S.C. § 631 et seq. The position of "magistrate judge" or "magistrate" also exists in some unrelated state courts. US magistrate judges generally oversee first appearances of criminal defendants, set bail, and conduct other administrative duties.

Formerly in ancient Rome, a magistratus was one of the highest ranking government officers, and possessed both judicial and executive powers. Today in common law systems, a magistrate has limited law enforcement and administration authority. In civil law systems, a magistrate may be a judge in a superior court, where the magistrates' court might have jurisdiction over civil and criminal cases. A related, but not always equivalent term, is chief magistrate which historically can denote a political and administrative officer.

Civil law is a branch of the law.[1] In common law countries such as England, Wales, and the United States, the term refers to non-criminal law.[1][2] The law relating to civil wrongs and quasi-contracts is part of the civil law.[3] The law of property is embraced by civil law.[4] Civil law can, like criminal law, be divided into substantive law and procedural law.[5] The rights and duties of individuals amongst themselves is the primary concern of civil law.[6] It is often suggested that civil proceedings are taken for the purpose of obtaining compensation for injury, and may thus be distinguished from criminal proceedings, whose purpose is to inflict punishment. However, exemplary or punitive damages may be awarded in civil proceedings. It was also formerly possible for common informers to sue for a penalty in civil proceedings.[7]

Because some courts have both civil and criminal jurisdiction, civil proceedings cannot be defined as those taken in civil courts.[8] In the United States, the expression "civil courts" is used as a "shorthand for trial courts in civil cases".[9]

In England, the burden of proof in civil proceedings is in general, with a number of exceptions such as committal proceedings for civil contempt, proof on a balance of probabilities.[10] In civil cases in the Maldives, the burden of proof requires the plaintiff to convince the court of the plaintiff's entitlement to the relief sought. This means that the plaintiff must prove each element of the claim, or cause of action in order to recover.[11]

Probability is the measure of the likelihood that an event will occur.[1] Probability is quantified as a number between 0 and 1 (where 0 indicates impossibility and 1 indicates certainty).[2][3] The higher the probability of an event, the more certain we are that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is unbiased, the two outcomes ("head" and "tail") are equally probable; the probability of "head" equals the probability of "tail." Since no other outcome is possible, the probability is 1/2 (or 50%) of either "head" or "tail". In other words, the probability of "head" is 1 out of 2 outcomes and the probability of "tail" is also, 1 out of 2 outcomes, expressed as 0.5 using the above mentioned quantification system.[2][3]

These concepts have been given an axiomatic mathematical formalization in probability theory (see probability axioms), which is used widely in such areas of study as mathematics, statistics, finance, gambling, science (in particular physics), artificial intelligence/machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.[4]

Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] In 1959, Arthur Samuel defined machine learning as a "Field of study that gives computers the ability to learn without being explicitly programmed".[2] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[3] Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs,[4]:2 rather than following strictly static program instructions.

Machine learning is closely related to (and often overlaps with) computational statistics; a discipline which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is unfeasible. Example applications include spam filtering, optical character recognition (OCR),[5] search engines and computer vision. Machine learning is sometimes conflated with data mining,[6] where the latter sub-field focuses more on exploratory data analysis and is known as unsupervised learning.[4]:vii[7]

Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction. These analytical models allow researchers, data scientists, engineers, and analysts to "produce reliable, repeatable decisions and results" and uncover "hidden insights" through learning from historical relationships and trends in the data.[8]

Learning to rank[1] or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems.[2] Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a permutation of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.

Document retrieval is defined as the matching of some stated user query against a set of free-text records. These records could be any type of mainly unstructured text, such as newspaper articles, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.

Document retrieval is sometimes referred to as, or as a branch of, text retrieval. Text retrieval is a branch of information retrieval where the information is stored primarily in the form of text. Text databases became decentralized thanks to the personal computer and the CD-ROM. Text retrieval is a critical area of study today, since it is the fundamental basis of all internet search engines.

In neuropsychology, linguistics and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forums, such as speech, signing, or writing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic.[1]

Neuropsychology studies the structure and function of the brain as they relate to specific psychological processes and behaviors. It's an experimental field of psychology that aims to understand how behavior and cognition are influenced by brain functioning and is concerned with the diagnosis and treatment of behavioral and cognitive effects of neurological disorders. Whereas classical neurology focuses on the physiology of the nervous system and classical psychology is largely divorced from it, neuropsychology seeks to discover how the brain correlates with the mind. It thus shares concepts and concerns with neuropsychiatry and with behavioral neurology in general. The term neuropsychology has been applied to lesion studies in humans and animals. It has also been applied to efforts to record electrical activity from individual cells (or groups of cells) in higher primates (including some studies of human patients).[1] It is scientific in its approach, making use of neuroscience, and shares an information processing view of the mind with cognitive psychology and cognitive science.

In practice, neuropsychologists tend to work in research settings (universities, laboratories or research institutions), clinical settings (involved in assessing or treating patients with neuropsychological problems), forensic settings or industry (often as consultants where neuropsychological knowledge is applied to product design or in the management of pharmaceutical clinical-trials research for drugs that might have a potential impact on CNS functioning).

